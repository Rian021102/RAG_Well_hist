{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n8zOo7FKHGqM8Jebj7KKTEYGtDaOwF0X\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "api_key = os.environ[\"MISTRAL_API_KEY\"]\n",
    "print(os.getenv(\"MISTRAL_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rianrachmanto/miniforge3/envs/mlp/lib/python3.9/site-packages/langchain_mistralai/embeddings.py:105: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. At 3954 m, observed STDP increased from 220 bar to 245 bar and WOB increased from 4 ton to 9 ton. Drillstring stalled out and couldn't regain rotating.\n",
      "2. Drilled 8 1/2\" hole from 3912 m to 3954 m, average ROP 24 m/hr, with maximum gas 2,95%. Gas observed in cablehead, megging not satisfied and had to ventilate gas out of cable.\n",
      "3. Drilled 8 1/2\" hole from 3496 m to 3499 m, with average ROP 25-30 m/hr, with maximum gas 2,77%.\n",
      "4. Drilled 8 1/2\" hole from 3499 m to 3532 m, with average ROP 25-30 m/hr, with maximum gas 1,9%.\n",
      "5. Drilled 12 1/4\" hole from 3260 m to section TD at 3299 m, with average ROP 11,3 m/hr, with maximum gas 0,05%. Observed 440 l/h losses while flowchecking well on T/T.\n",
      "6. Drilled 17 1/2\" hole from 2372 m to 2418 m, with average ROP 11,5 m/hr, with maximum gas 0,05%. Pulled spool piece to rig floor and Layed out same.\n",
      "7. Drilled 8 1/2\" hole from 3974 m to 4025 m, with 2400 lpm, 220 bar, 120 rpm, 18-23 kNm, 4-5 ton WOB. Avg ROP 22 m/hr. ECD 1,53 SG. Max gas: 3,0 %.\n",
      "8. Drilled 8 1/2\" hole from 3954 m to 3974 m, with 2400 lpm, 220 bar, 120 rpm, 18-23 kNm, 4 ton WOB. Avg ROP 16 m/hr. ECD 1,51 SG. Max gas: 0,8 %.\n",
      "\n",
      "Please note that the context only mentions the drilling progress and some specific measurements, it does not explicitly mention any problem, but I listed the above as potential problems based on the given data.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_mistralai.embeddings import MistralAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# Load data\n",
    "loader = TextLoader(\"/Users/rianrachmanto/miniforge3/project/RAG_Drill_Report/data/remark.txt\")\n",
    "docs = loader.load()\n",
    "# Split text into chunks \n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "# Define the embedding model\n",
    "embeddings = MistralAIEmbeddings(model=\"mistral-embed\", mistral_api_key=api_key)\n",
    "# Create the vector store \n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "# Define a retriever interface\n",
    "retriever = vector.as_retriever()\n",
    "# Define LLM\n",
    "model = ChatMistralAI(mistral_api_key=api_key)\n",
    "# Define prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "# Create a retrieval chain to answer questions\n",
    "document_chain = create_stuff_documents_chain(model, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "response = retrieval_chain.invoke({\"input\": \"What are the problems encountered during drilling? List it based on depth\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
